{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1sfpHcHjnsD"
      },
      "source": [
        "# Backpropagation\n",
        "The backpropagation algorithm is useful to find the weights and biases of a Multi-Layer Perceptron (MLP). It employs the chain rule of the derivative to find the direction in which the weight values need to be modified in order to minimize the error at the output. The algorithm performs in two phases: information is first propagated forward to compute the output of the network for a given input, and the error of the network; then, the error signal is propagated backwards and the network parameters are modified accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GeN8OjUjnsG"
      },
      "source": [
        "## Loading the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWJ0kOXOjnsG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as pl\n",
        "from ipywidgets import interact, widgets\n",
        "\n",
        "from matplotlib import animation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_tAXA0ajnsH"
      },
      "source": [
        "## Loading the Perceptron code\n",
        "In order to the make this nothebook smaller, some of the functions (activation functions, and some of the code allowing the visualization of the results) was implemented in a separate python file. You are free to open it if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**For it to work on Colab, make sure to have uploaded the mlp.py file in your environment**</font>"
      ],
      "metadata": {
        "id": "6OOYWZJfjzQT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niWR9PA7jnsI"
      },
      "outputs": [],
      "source": [
        "import mlp as mlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymXAo09PjnsI"
      },
      "source": [
        "## The Dataset\n",
        "The following script allows you to create a 2D dataset by using the mouse. The left click adds points belonging to class A (blue), and the right click adds points belonging to class B (red). You can create as many points as you desire. The final dataset will contain hence three values per point: x coordinate (-1 ≤ x ≤ 1), y coordinate (-1 ≤ y ≤ 1) and the class ∈ {1,-1}."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipympl"
      ],
      "metadata": {
        "id": "26y300YbkJFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**For it to work on Colab, you will need to reload your session (Exécution -> redémarrer la session)**</font>"
      ],
      "metadata": {
        "id": "cTVAwOsYkNG7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pRnht0mjnsJ"
      },
      "outputs": [],
      "source": [
        "%matplotlib widget\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "fig = pl.figure(figsize=(6,6))\n",
        "pl.title(\"Input Dataset\")\n",
        "pl.xlim((-1.2,1.2))\n",
        "pl.ylim((-1.2,1.2))\n",
        "\n",
        "dataset = []\n",
        "\n",
        "def on_press(event):\n",
        "    if event.key == 'b':\n",
        "        dataset.append((event.xdata, event.ydata, -1))\n",
        "        pl.scatter(event.xdata, event.ydata, color='blue')\n",
        "        pl.draw()\n",
        "    elif event.key == 'r':\n",
        "        dataset.append((event.xdata, event.ydata, 1))\n",
        "        pl.scatter(event.xdata, event.ydata, color='red')\n",
        "        pl.draw()\n",
        "\n",
        "# Attach the event handler\n",
        "fig.canvas.mpl_connect('key_press_event', on_press);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0s63DrgjnsK"
      },
      "source": [
        "This is the dataset you just created. Check that there are no NaNs in the third column. Create once again the dataset if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dl2ayipjnsK"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron"
      ],
      "metadata": {
        "id": "87YwDHPqsxPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "            ______________\n",
        "           /              \\\n",
        "x __w_x__ j                l\n",
        "  _______ | f_act(I.W + b) |----- output\n",
        "y   w_y   l                j\n",
        "           \\______________/\n",
        "Where:\n",
        "x = input x\n",
        "y = input y\n",
        "b = bias\n",
        "f_act = activation function\n",
        "I = vector of inputs [x, y]\n",
        "W = vector of weights [w_x, w_y]\n",
        "```\n",
        "\n",
        "$$output = f\\_act(\\sum_{i=0}^{1}{(I_{i} * W_{i}) + b})$$"
      ],
      "metadata": {
        "id": "-qDA2r-NtKlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron(input_values, weights, bias, activation_function):\n",
        "    '''Computes the output of a perceptron\n",
        "    :param input_values: inputs to the perceptron\n",
        "    :param weights: perceptron parameters (multiply inputs)\n",
        "    :param bias: perceptron parameter (adds to inputs)\n",
        "    :param activation_function: activation function to apply to the weighted sum of inputs\n",
        "    :return: perceptron output'''\n",
        "    neta = np.dot(input_values, weights) + bias\n",
        "    output = activation_function(neta)\n",
        "    return output"
      ],
      "metadata": {
        "id": "cU_ekSKLsrbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multilayer Perceptron"
      ],
      "metadata": {
        "id": "zujBWTjyrNA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "                 _________________\n",
        "                /                 \\\n",
        "x _____w_x_0___j                   l   w_h_0\n",
        "     \\        _| f_act(I.W0 + b_0) |-----.\n",
        "      w_x_1  / l                   j     |      _________________\n",
        "        \\   /   \\_________________/      |     /                 \\\n",
        "         \\ /                           h0|____j                   l\n",
        "          \\                               ____| f_act(H.Wh + b_h) |------ output\n",
        "         / \\     _________________     h1|    l                   j\n",
        "    w_y_0   \\   /                 \\      |     \\_________________/\n",
        "      /      \\_j                   l     |\n",
        " ____/__w_y_1__| f_act(I.W1 + b_1) |-----'\n",
        "y              l                   j   w_h_1\n",
        "                \\_________________/\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "x = input x\n",
        "\n",
        "y = input y\n",
        "\n",
        "b_0 = bias neuron 0\n",
        "\n",
        "b_1 = bias neuron 1\n",
        "\n",
        "b_h = bias output neuron\n",
        "\n",
        "f_act = activation function\n",
        "\n",
        "I = vector of inputs [x, y]\n",
        "\n",
        "H = vector of hidden activations [h0, h1]\n",
        "\n",
        "W0 = vector of weights [w_x_0, w_y_0]\n",
        "\n",
        "W1 = vector of weights [w_x_1, w_y_1]\n",
        "\n",
        "Wh = vector of weights [w_h_0, w_h_1]\n"
      ],
      "metadata": {
        "id": "kGXJo9tQq-Si"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$h0 = f\\_act(\\sum_{i=0}^{1}{(I_{i} * W0_{i}) + b\\_0})$$\n",
        "$$h1 = f\\_act(\\sum_{i=0}^{1}{(I_{i} * W1_{i}) + b\\_1})$$\n",
        "$$output = f\\_act(\\sum_{i=0}^{1}{(H_{i} * Wh_{i}) + b\\_h})$$"
      ],
      "metadata": {
        "id": "pL16exaVsL8E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kknuAjXrjnsL"
      },
      "source": [
        "## Finding the weights by hand\n",
        "In this section you should try to find the set of weights that makes a MLP to separate the distribution of the two classes you previously defined. Use the sliders to modify the value of each one of the connections and the biases of the MLP while trying to separate the two classes (blue and red). The curve on the right represents the classification error (MSE). If the modifications you provide improve the classification, you should see the error to decrease."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEA_8XuojnsM"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "plotter = mlp.MLPPlotter2D(data=np.asarray(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbNIbjhTjnsM"
      },
      "outputs": [],
      "source": [
        "fig.clf()\n",
        "_= interact(plotter.plot_interactive, **plotter.controls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IbVDaKsjnsN"
      },
      "source": [
        "## The Backpropagation algorithm\n",
        "Instead of trying to find the network parameters by hand, we propose to use the backpropagation algorithm. This section shows some details of its implementation. Look at the code in compute_delta_w and try to understand it."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "p1Xi5HX2jnsN"
      },
      "source": [
        "                 _________________\n",
        "                /                 \\\n",
        "x _____w_x_0___j                   l   w_h_0\n",
        "     \\        _| f_act(I.W0 + b_0) |-----.\n",
        "      w_x_1  / l                   j     |      _________________\n",
        "        \\   /   \\_________________/      |     /                 \\\n",
        "         \\ /                           h0|____j                   l\n",
        "          \\                               ____| f_act(H.Wh + b_h) |------ output\n",
        "         / \\     _________________     h1|    l                   j\n",
        "    w_y_0   \\   /                 \\      |     \\_________________/\n",
        "      /      \\_j                   l     |\n",
        " ____/__w_y_1__| f_act(I.W1 + b_1) |-----'\n",
        "y              l                   j   w_h_1\n",
        "                \\_________________/\n",
        "\n",
        "Where:\n",
        "x = input x\n",
        "y = input y\n",
        "b_0 = bias neuron 0\n",
        "b_1 = bias neuron 1\n",
        "b_h = bias output neuron\n",
        "f_act = activation function\n",
        "I = vector of inputs [x, y]\n",
        "H = vector of hidden activations [h0, h1]]\n",
        "W0 = vector of weights [w_x_0, w_y_0]\n",
        "W1 = vector of weights [w_x_1, w_y_1]\n",
        "Wh = vector of weights [w_h_0, w_h_1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaGrClIyjnsO"
      },
      "source": [
        "### Derivation using the chain rule\n",
        "\n",
        "$$ Error = \\frac{1}{2} (output - target)^2 $$\n",
        "$$ \\Delta w = - \\alpha \\cdotp \\frac{\\partial Error}{\\partial w} $$\n",
        "\n",
        "Output layer\n",
        "$$ output = f\\_act(neta\\_h) $$\n",
        "$$ neta\\_h = (w\\_h\\_0 \\cdotp h0) + (w\\_h\\_1 \\cdotp h1) + b_h $$\n",
        "\n",
        "$$ \\frac{\\partial Error}{\\partial w\\_h\\_i} = \\frac{\\partial Error}{\\partial output} \\cdotp \\frac{\\partial output}{\\partial neta\\_h} \\cdotp \\frac{\\partial neta\\_h}{\\partial w\\_h\\_i} $$\n",
        "$$ \\frac{\\partial Error}{\\partial w\\_h\\_i} = (output - target) \\cdotp f\\_act'(neta\\_h) \\cdotp hi $$\n",
        "\n",
        "$$ \\frac{\\partial Error}{\\partial b\\_i} = \\frac{\\partial Error}{\\partial output} \\cdotp \\frac{\\partial output}{\\partial neta\\_h} \\cdotp \\frac{\\partial neta\\_h}{\\partial b\\_i} $$\n",
        "$$ \\frac{\\partial Error}{\\partial b\\_h} = (output - target) \\cdotp f\\_act'(neta\\_h) \\cdotp 1 $$\n",
        "\n",
        "Hidden layer\n",
        "$$ hi = f\\_act(neta\\_i) $$\n",
        "$$ neta\\_i = (x \\cdotp w\\_x\\_i) + (y \\cdotp w\\_y\\_i) + b_i $$\n",
        "\n",
        "$$ \\frac{\\partial Error}{\\partial w\\_x\\_i} = \\frac{\\partial Error}{\\partial output} \\cdotp \\frac{\\partial output}{\\partial neta\\_h} \\cdotp \\big( \\frac{\\partial neta\\_h}{\\partial h\\_i} \\cdotp \\frac{\\partial h\\_i}{\\partial neta\\_i} \\cdotp \\frac{\\partial neta\\_i}{\\partial w\\_x\\_i} \\big) $$\n",
        "$$ \\frac{\\partial Error}{\\partial w\\_x\\_i} = (output - target) \\cdotp f\\_act'(neta\\_h) \\cdotp \\big( w\\_h\\_i \\cdotp f\\_act'(neta\\_i) \\cdotp x \\big) $$\n",
        "\n",
        "$$ \\frac{\\partial Error}{\\partial w\\_y\\_i} = \\frac{\\partial Error}{\\partial output} \\cdotp \\frac{\\partial output}{\\partial neta\\_h} \\cdotp \\big( \\frac{\\partial neta\\_h}{\\partial h\\_i} \\cdotp \\frac{\\partial h\\_i}{\\partial neta\\_i} \\cdotp \\frac{\\partial neta\\_i}{\\partial w\\_y\\_i} \\big) $$\n",
        "$$ \\frac{\\partial Error}{\\partial w\\_y\\_i} = (output - target) \\cdotp f\\_act'(neta\\_h) \\cdotp \\big( w\\_h\\_i \\cdotp f\\_act'(neta\\_i) \\cdotp y \\big) $$\n",
        "\n",
        "$$ \\frac{\\partial Error}{\\partial b\\_i} = \\frac{\\partial Error}{\\partial output} \\cdotp \\frac{\\partial output}{\\partial neta\\_h} \\cdotp \\big( \\frac{\\partial neta\\_h}{\\partial h\\_i} \\cdotp \\frac{\\partial h\\_i}{\\partial neta\\_i} \\frac{\\partial neta\\_i}{\\partial b\\_i} \\big) $$\n",
        "$$ \\frac{\\partial Error}{\\partial b\\_i} = (output - target) \\cdotp f\\_act'(neta\\_h) \\cdotp \\big( w\\_h\\_i \\cdotp f\\_act'(neta\\_i) \\cdotp 1 \\big) $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWX1JKy2jnsP"
      },
      "outputs": [],
      "source": [
        "def compute_delta_w(input_values, targets, alpha, activation_function, weights, bias):\n",
        "    x = input_values[:,0]\n",
        "    y = input_values[:,1]\n",
        "\n",
        "    w_x_0 = weights[0]\n",
        "    w_x_1 = weights[1]\n",
        "    w_y_0 = weights[2]\n",
        "    w_y_1 = weights[3]\n",
        "    w_h_0 = weights[4]\n",
        "    w_h_1 = weights[5]\n",
        "    b_0   = bias[0]\n",
        "    b_1   = bias[1]\n",
        "    b_h   = bias[2]\n",
        "\n",
        "    # feedforward\n",
        "    h_0, h_0_d = perceptron(input_values, [w_x_0, w_y_0], b_0, activation_function)\n",
        "    h_1, h_1_d = perceptron(input_values, [w_x_1, w_y_1], b_1, activation_function)\n",
        "    h = np.array([h_0, h_1]).T\n",
        "    output, output_d = perceptron(h, [w_h_0, w_h_1], b_h, activation_function)\n",
        "\n",
        "    #output layer\n",
        "    error = output - targets\n",
        "    d_w_h_0 = -alpha * (error * output_d * h_0)\n",
        "    d_w_h_1 = -alpha * (error * output_d * h_1)\n",
        "    d_b_h   = -alpha * (error * output_d)\n",
        "\n",
        "    #hidden layer\n",
        "    d_w_x_0 = -alpha * (error * output_d) * (w_h_0 * h_0_d * x)\n",
        "    d_w_x_1 = -alpha * (error * output_d) * (w_h_1 * h_1_d * x)\n",
        "    d_w_y_0 = -alpha * (error * output_d) * (w_h_0 * h_0_d * y)\n",
        "    d_w_y_1 = -alpha * (error * output_d) * (w_h_1 * h_1_d * y)\n",
        "    d_b_0 = -alpha * (error * output_d) * (w_h_0 * h_0_d)\n",
        "    d_b_1 = -alpha * (error * output_d) * (w_h_1 * h_1_d)\n",
        "\n",
        "    return (np.vstack((d_w_x_0, d_w_x_1, d_w_y_0, d_w_y_1, d_w_h_0, d_w_h_1)).T, np.vstack((d_b_0, d_b_1, d_b_h)).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll22S6esjnsP"
      },
      "source": [
        "## Batch learning\n",
        "In this section you are going to use the backpropagation algorithm to find the weights of a MLP that solves the classification problem you have previously defined. When you launch the cell, the weights and the bias are initialized at random, and the algorithm perform some iterations (NUMBER_OF_EPOCHS) doing the following:\n",
        "- for each point in the dataset, compute the modifications ($\\Delta w$) to be done at each parameter in order to minimize the error function\n",
        "- sum up all the modifications -> batch policy\n",
        "- modify the weights and biases of the MLP\n",
        "\n",
        "The cell records the effects of the modifications performed in a video. Therefore, you can visualize the learning process afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "JDvXs3UWjnsP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "inputs = np.asarray(dataset)[:,0:2]\n",
        "targets = np.asarray(dataset)[:,2]\n",
        "weights = np.random.normal(size=6)\n",
        "bias = np.random.normal(size=3)\n",
        "activation_function = mlp.htan\n",
        "\n",
        "ALPHA = 0.05\n",
        "NUMBER_OF_EPOCHS = 100\n",
        "\n",
        "fig = pl.figure(figsize=(12, 4))\n",
        "plotter = mlp.MLPPlotter2D(data=np.asarray(dataset))\n",
        "plotter.init_animation()\n",
        "\n",
        "def run_epoch_batch(i, alpha, inputs, targets, activation_function, weights, bias):\n",
        "    d_weights, d_bias = compute_delta_w(inputs, targets, ALPHA, activation_function, weights, bias)\n",
        "    weights += np.sum(d_weights, axis=0)\n",
        "    bias += np.sum(d_bias, axis=0)\n",
        "\n",
        "    return plotter.data2animation(i, inputs, weights, bias, targets, activation_function)\n",
        "\n",
        "SHOW_VIDEO = True\n",
        "if SHOW_VIDEO:\n",
        "    anim = animation.FuncAnimation(fig, run_epoch_batch, fargs=(ALPHA, inputs, targets, activation_function, weights, bias), frames=NUMBER_OF_EPOCHS, interval=20, blit=True)\n",
        "    mlp.display_animation(anim)\n",
        "else:\n",
        "    for i in np.arange(NUMBER_OF_EPOCHS):\n",
        "        run_epoch_batch(i, ALPHA, inputs, targets, activation_function, weights, bias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUtS5VCbjnsQ"
      },
      "source": [
        "## Exercise\n",
        "You are free to modify the learning rate (ALPHA) and the number of iterations (NUMBER_OF_EPOCHS).\n",
        "\n",
        "Try different 2D classification problems and observe the behaviour of the algorithm in terms of:\n",
        "- Learning rate needed\n",
        "- Convergence speed\n",
        "- Oscillations\n",
        "\n",
        "Bare in mind that, in the current implementation, the parameters (weights and bias) are initialized randomly every time you launch the cell\n",
        "\n",
        "1. What happens if the boundaries between both classes are well defined?\n",
        "![separable](https://drive.google.com/uc?id=1jctNojH56KXougUipA8fney25xwFeH8l)\n",
        "\n",
        "2. What happens if the classes overlap? What could you say about oscillations in the error signal?\n",
        "![overlapping](https://drive.google.com/uc?id=1pNjh1OQjJgQw_UTEzancv-xEnrKubTfl)\n",
        "\n",
        "3. What happens if it is not possible to separate the classes with a single line? What could you say about local minima?\n",
        "![non_separable](https://drive.google.com/uc?id=17QWBDNiw2TuY7EIoH1BoDzDc2YEIm__M)\n",
        "\n",
        "4. What happens if the points of one of the classes are separated in subgroups (blobs)?\n",
        "![blobs](https://drive.google.com/uc?id=1_BZqzGz4yYIDiv54t4I0Sax8aUZp7qOX)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}